{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online CNMF-E\n",
    "\n",
    "This demo shows an example of doing online analysis on one-photon data. We compare offline and online approaches. The dataset used is courtesy of the Miniscope project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().magic('load_ext autoreload')\n",
    "get_ipython().magic('autoreload 2')\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "import glob\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy\n",
    "\n",
    "logging.basicConfig(format=\n",
    "                          \"%(relativeCreated)12d [%(filename)s:%(funcName)10s():%(lineno)s] [%(process)d] %(message)s\",\n",
    "                    # filename=\"/tmp/caiman.log\",\n",
    "                    level=logging.WARNING)\n",
    "\n",
    "import caiman as cm\n",
    "from caiman.source_extraction import cnmf as cnmf\n",
    "from caiman.motion_correction import MotionCorrect\n",
    "from caiman.utils.utils import download_demo\n",
    "import matplotlib.pyplot as plt\n",
    "from caiman.utils.visualization import nb_inspect_correlation_pnr\n",
    "import holoviews as hv\n",
    "import bokeh.plotting as bpl\n",
    "\n",
    "bpl.output_notebook()\n",
    "hv.notebook_extension('bokeh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select file(s) to be processed\n",
    "The `download_demo` function will download the specific file for you and return the complete path to the file which will be stored in your `caiman_data` directory. If you adapt this demo for your data make sure to pass the complete path to your file(s). Remember to pass the `fnames` variable as a list. Note that the memory requirement of the offline CNMF-E algorithm are much higher compared to the standard CNMF algorithm. One of the benefits of the online approach is the reduced memory requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = [download_demo('msCam13.avi')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch (offline) approach\n",
    "\n",
    "We start with motion correction and then proceed with the source extraction using the CNMF-E algorithm. For a detailed 1p demo check `demo_pipeline_cnmfE.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# motion correction parameters\n",
    "motion_correct = True            # flag for performing motion correction\n",
    "pw_rigid = False                 # flag for performing piecewise-rigid motion correction (otherwise just rigid)\n",
    "gSig_filt = (7, 7)               # size of high pass spatial filtering, used in 1p data\n",
    "max_shifts = (15, 15)            # maximum allowed rigid shift\n",
    "strides = (96, 96)               # start a new patch for pw-rigid motion correction every x pixels\n",
    "overlaps = (24, 24)              # overlap between pathes (size of patch strides+overlaps)\n",
    "border_nan = 'copy'              # replicate values along the boundaries\n",
    "\n",
    "mc_dict = {\n",
    "    'pw_rigid': pw_rigid,\n",
    "    'max_shifts': max_shifts,\n",
    "    'gSig_filt': gSig_filt,\n",
    "    'strides': strides,\n",
    "    'overlaps': overlaps,\n",
    "    'border_nan': border_nan\n",
    "}\n",
    "\n",
    "opts = cnmf.params.CNMFParams(params_dict=mc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% start a cluster for parallel processing (if a cluster already exists it will be closed and a new session will be opened)\n",
    "if 'dview' in locals():\n",
    "    cm.stop_server(dview=dview)\n",
    "c, dview, n_processes = cm.cluster.setup_cluster(\n",
    "    backend='local', n_processes=None, single_thread=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = MotionCorrect(fnames, dview=dview, **opts.get_group('motion'))\n",
    "mc.motion_correct(save_movie=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inspect motion correction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_results = True\n",
    "if inspect_results:\n",
    "    cm.concatenate((cm.load(fnames), cm.load(mc.mmap_file)), axis=1).play()\n",
    "plt.figure(); plt.plot(mc.shifts_rig); plt.legend(['x-shifts', 'y-shifts'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The motion correction results look good. We then proceed with memory mapping and checking the correlation/pnr images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "fname_new = cm.save_memmap(mc.mmap_file, base_name='memmap_', order='C',\n",
    "                           border_to_0=0, dview=dview)\n",
    "Yr, dims, T = cm.load_memmap(fname_new)\n",
    "images = Yr.T.reshape((T,) + dims, order='F')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect correlation and PNR images to set relevant thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gSig = (5, 5)\n",
    "\n",
    "cn_filter, pnr = cm.summary_images.correlation_pnr(images[::max(T//1000, 1)], gSig=gSig[0], swap_dim=False) # change swap dim if output looks weird, it is a problem with tiffile\n",
    "# inspect the summary images and set the parameters\n",
    "\n",
    "nb_inspect_correlation_pnr(cn_filter, pnr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters for source extraction\n",
    "From the images above we select `min_pnr = 6` and `min_corr = 0.8`. We pass these alongside the other parameters needed for offline 1p processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "min_pnr = 6\n",
    "min_corr = 0.8\n",
    "rf = 48                                        # half size of each patch\n",
    "stride = 8                                     # amount of overlap between patches     \n",
    "ssub = 1                                       # spatial downsampling factor   \n",
    "decay_time = 0.4                               # length of typical transient (in seconds)\n",
    "fr = 10                                        # imaging rate (Hz) \n",
    "gSig = (5, 5)                                  # expected half size of neurons\n",
    "gSiz = (15, 15)                                # half size for neuron bounding box   \n",
    "p = 0                                          # order of AR indicator dynamics\n",
    "min_SNR = 1.5                                  # minimum SNR for accepting new components\n",
    "rval_thr = 0.85                                # correlation threshold for new component inclusion\n",
    "merge_thr = 0.65                               # merging threshold\n",
    "K = None                                       # initial number of components\n",
    "\n",
    "cnmfe_dict = {'fnames': fnames,\n",
    "              'fr': fr,\n",
    "              'decay_time': decay_time,\n",
    "              'method_init': 'corr_pnr',\n",
    "              'gSig': gSig,\n",
    "              'gSiz': gSiz,\n",
    "              'rf': rf,\n",
    "              'stride': stride,\n",
    "              'p': p,\n",
    "              'nb': 0,\n",
    "              'ssub': ssub,\n",
    "              'min_SNR': min_SNR,\n",
    "              'min_pnr': min_pnr,\n",
    "              'min_corr': min_corr,\n",
    "              'bas_nonneg': False,\n",
    "              'center_psf': True,\n",
    "              'rval_thr': rval_thr,\n",
    "              'only_init': True,\n",
    "              'merge_thr': merge_thr,\n",
    "              'K': K}\n",
    "opts.change_params(cnmfe_dict);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "t1 = -time()\n",
    "cnm = cnmf.CNMF(n_processes=n_processes, dview=dview, params=opts)\n",
    "cnm.fit(images)\n",
    "t1 += time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cnm.estimates.plot_contours_nb(img=pnr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnm.estimates.hv_view_components(img=cn_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show a movie with the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnm.estimates.play_movie(images, magnification=0.75, include_bck=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online Processing\n",
    "Now try the online approach. The idea behind the online algorithm is simple:\n",
    "- First initialize the estimates by running the batch (offline) algorithm in small subset.\n",
    "- Then process each frame as it arrives. The processing consists of:\n",
    "    * Motion correct the new frame\n",
    "    * Extract the activity of existing neurons at this frame, and neuropil\n",
    "    * Search for new neurons that appear in this frame and have not been detected earlier.\n",
    "- Periodically update shapes of existing neurons and background model.\n",
    "\n",
    "## Setup additional parameters for online processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "online_opts = deepcopy(cnm.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf = 48\n",
    "stride = 8\n",
    "ssub = 1\n",
    "ds_factor = 2*ssub\n",
    "\n",
    "gSig = (6//ds_factor, 6//ds_factor)            # expected half size of neurons\n",
    "gSiz = (15//ds_factor, 15//ds_factor)\n",
    "sniper_mode = False                            # flag using a CNN to detect new neurons (o/w space correlation is used)\n",
    "init_batch = 300                               # number of frames for initialization (presumably from the first file)\n",
    "expected_comps = 500                           # maximum number of expected components used for memory pre-allocation (exaggerate here)\n",
    "dist_shape_update = False                      # flag for updating shapes in a distributed way\n",
    "min_num_trial = 5                              # number of candidate components per frame     \n",
    "K = None                                       # initial number of components\n",
    "epochs = 1                                     # number of passes over the data\n",
    "show_movie = False                             # show the movie with the results as the data gets processed\n",
    "\n",
    "online_dict = {'epochs': epochs,\n",
    "               'nb': 0,\n",
    "               'ssub': ssub,\n",
    "               'ds_factor': ds_factor,                                   # ds_factor >= ssub should hold\n",
    "               'gSig': gSig,\n",
    "               'gSiz': gSiz,\n",
    "               'gSig_filt': (3, 3),\n",
    "               'min_corr': min_corr,\n",
    "               'bas_nonneg': False,\n",
    "               'center_psf': True,\n",
    "               'max_shifts_online': 15,\n",
    "               'rval_thr': rval_thr,\n",
    "               'init_batch': init_batch,\n",
    "               'only_init': True,\n",
    "               'init_method': 'cnmf',\n",
    "               'normalize_init': False,\n",
    "               'update_freq': 200,\n",
    "               'expected_comps': expected_comps,\n",
    "               'sniper_mode': sniper_mode,\n",
    "               'dist_shape_update' : dist_shape_update,\n",
    "               'min_num_trial': min_num_trial,\n",
    "               'epochs': epochs,\n",
    "               'show_movie': show_movie}\n",
    "online_opts.change_params(online_dict);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnm_online = cnmf.online_cnmf.OnACID(params=online_opts, dview=dview)\n",
    "cnm_online.fit_online()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#images = cm.load(fnames[0], subindices=slice(0,1000))\n",
    "#Cn, pnr = cm.summary_images.correlation_pnr(images[::1], gSig=gSig[0], swap_dim=False) # change swap dim if output looks weird, it is a problem with tiffile\n",
    "cnm_online.estimates.nb_view_components(img=pnr, denoised_color='red');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnm_online.estimates.plot_contours_nb(img=pnr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot timing\n",
    "The plot below shows the time spent on each part of the algorithm (motion correction, tracking of current components, detect new components, update shapes) for each frame. Note that if you displayed a movie while processing the data (`show_movie=True`) the time required to generate this movie will be included here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "T_motion = 1e3*np.array(cnm_online.t_motion)\n",
    "T_detect = 1e3*np.array(cnm_online.t_detect)\n",
    "T_shapes = 1e3*np.array(cnm_online.t_shapes)\n",
    "T_online = 1e3*np.array(cnm_online.t_online) - T_motion - T_detect - T_shapes\n",
    "plt.figure()\n",
    "plt.stackplot(np.arange(len(T_motion)), T_motion, T_online, T_detect, T_shapes)\n",
    "plt.legend(labels=['motion', 'process', 'detect', 'shapes'], loc=2)\n",
    "plt.title('Processing time allocation')\n",
    "plt.xlabel('Frame #')\n",
    "plt.ylabel('Processing time [ms]')\n",
    "plt.ylim([0, 1.2e3*np.percentile(np.array(cnm_online.t_online), 90)]);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
